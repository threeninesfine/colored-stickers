---
title: "Completely Separated? Evaluating Simulations for Logistic Regression Models"
author: "Michael Flanagin  \\ flanna@uw.edu"
date: "7/23/2018"
classoption: portrait
header-includes:
  - \usepackage[table]{xcolor}
  - \usepackage{booktabs}
  - \usepackage{caption}
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    number_sections: yes
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: cerulean
csl: mee.csl
bibliography: references.bib
---
\captionsetup[table]{ labelformat = empty } <!-- disable auto table numbering with kable -->

```{r setup, eval = FALSE, echo=FALSE}
  devtools::install_github("cboettig/knitcitations@v1")
  library(knitcitations); cleanbib()
  cite_options(citation_format = "pandoc", check.entries=FALSE)
  library(bibtex)
```

```{r globalopts, cache = TRUE, warning = FALSE, echo = FALSE}
# Load libraries necessary for functions used in this document
library('knitr')
library('xtable')
library('kableExtra')  # for additional table formatting
# library('dplyr')  # for table coloring
# Set global chunk options
opts_chunk$set(echo = FALSE,     # do NOT repeat code in final document
               message = FALSE,  # do NOT print R messages in document
               warning = FALSE,  # do NOT print warnings
               self.contained = TRUE ) # do NOT call other .Rnw docs! 
round_digits <- 3  # digits to round to when presenting results
options( knitr.table.format = "latex")  # guarantees 'kableExtra' formatting handled properly
options( scipen = 999 )
#         knitr.kable.na = " - ", 
#         scipen = 10)  # replaces NA entries with ' - ' in kable output
#         scipen = 999 )  # effectively disables scientific notation
setwd(getwd())  # set working directory to current file path.

# function kable_booktabs_linesep()
kable_booktabs_linesep <- function( nlines = 6 ){
  #' input: 
  #'   nlines := number of lines per block of table rows
  #' output:
  #'   [character] linespace string to pass to kable()
  #' example:
  #'   kable( foo_table, booktabs = TRUE, linesep = kable_booktabs_linesep( nlines = 6) )
  return(c(rep( '', nlines - 1 ), "\\addlinespace"))
}
```

# The Issue
Four simulation settings are considered on the basis of variable type (binary or continuous) for outcomes and prognostic factors.
They are referred to as:

* Batch 1: binary outcome (Y), binary prognostic factors (X)
* Batch 2: binary outcome (Y), continuous prognostic factors (X)
* Batch 3: continuous outcome (Y), binary prognostic factors (X)
* Batch 4: continuous outcome (Y), continuous prognostic factors (X)

For binary outcomes (Batches 1 and 2) under low outcome prevalence settings, one treatment group may end up having all observed events (thus the prognostic factor perfectly predicts the outcomes).
In this case (dubbed _complete separation_), the logistic regression optimization fails to converge.
In __R__ the glm() function returns inflated estimates and standard errors.

We first chose to identify these cases using a threshold standard error of 1000, but the resulting bias tables (see "Tabulating Results" report as of 18 July 2018) indicate we need to do more to characterize simulations exhibiting complete (or partial) separation that result in inflated estimates and standard errors.

*******************************************************************************

# The Data
We load in the simulation results from both Batch 1 and 2. 
Note: these files are large (greater than 1gb in some cases)

```{r load_datasets, cache = TRUE, echo = FALSE}
library("simulator")

#' [ 'results_directory' contains folder 'files' with .Rdata model, draw, output, evals ] 
results_directory <- "/Users/Moschops/Documents/MSThesis/datasets/results"

#' [ Batch 1 of 4: binary Y, binary X ] 
batch1 <- load_simulation( name = "alloc-simulation-batch-1-of-4", dir = results_directory )
#' [ Batch 2 of 4: binary Y, continuous X ] 
batch2 <- load_simulation( name = "alloc-simulation-batch-2-of-4", dir = results_directory )

```

Output is classified by:

* allocation method
  + Complete randomization ("CR"),
  + Stratified block randomization("SBR"),
  + Covariate adaptive allocation, probability of biased allocation of 0.70 ("CAA-MI-2-PBA-0.70")
  + Covariate adaptive allocation, probability of biased allocation of 1.00 ("CAA-MI-2-PBA-1.00")

```{r output-method-names, cache = TRUE, echo = FALSE}
#' List of valid output method names
alloc_method <- c("CR", "SBR", "CAA-MI-2-PBA-0.70", "CAA-MI-2-PBA-1.00");
analysis_method <- c("REG", "RERAND");
adjustment <- c("ADJ", "UN")

pastey <- function( ... ){ paste( ..., sep = "_")}
method_names_short <- do.call(pastey, expand.grid( alloc_method[1:3], analysis_method[1], adjustment ))
method_names_rerand <- do.call(pastey, expand.grid( alloc_method[3], analysis_method[2], adjustment ))
method_names_determ <- do.call(pastey, expand.grid( alloc_method[4], analysis_method, adjustment ))
###############################################################################
methods_to_use <- c( method_names_short, method_names_rerand, method_names_determ )

```

*******************************************************************************

# Processing Results: One Example

We loop through each model in the simulation, processing the results.
Below is an example where we consider the first model (and loop through the remainder of the models after). 
Note: each output() call takes at least 120 seconds.
```{r process-results, eval = TRUE, cache = TRUE, echo = FALSE}
sim_j <- 1;  # only do the first iteration of the loop
verbose <- FALSE  # display simulation information as well as runtimes?
if( verbose ){
  cat(paste0("[ Model ", sim_j, " ][-|       ] Loading output from simulation [ ", batch1@name, " ]...\n")); ptm.all <- proc.time()
}
tryCatch({
  output_j <- output( batch1, methods = methods_to_use )[[ sim_j ]] #' Model sim_j, 
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n"); next})
method_names_output <- sapply( output_j, function( .object ){ .object@method_name }) 
#' simulation model has these outputs:
cbind( 1:length( method_names_output ), method_names_output )
```

```{r approach-1-output-to-df, eval = TRUE, cache = TRUE, echo = FALSE}
# Converting output objects to data frames (example 1, consider the runtimes!)
cat(paste0("Approach 1: Loading output into data frames:\n")); ptm <- proc.time();
dfs <- list();
for( out.index in 1:length( output_j )){
  dfs[[ out.index ]]  <- data.frame(t(vapply( output_j[[ out.index ]]@out, function( .list ){ unlist( .list[1:9] )}, numeric(9))))
  dimnames(dfs[[ out.index ]])[[2]] <- c("est", "se", "t", "p", "adjusted", "rerandomized", 
                                         "cilower", "ciupper", "num_rerandomizations")
}
cat(paste0("Success! \nElapsed time: \n")); print( proc.time() - ptm );
```

```{r approach-2-output-to-df, eval = TRUE, cache = TRUE, echo = FALSE}
# Converting output objects to data frames (approach 2, using matrices)
cat(paste0("Approach 2: Loading output into matrices:\n")); ptm <- proc.time();
var_labels <- c("est", "se", "t", "p", "adjusted", "rerandomized", "cilower", "ciupper", "num_rerandomizations")
nsim <- 5010;
# Given the above dimensions, initialize 'outmat' matrix beforehand.
outmats <- lapply( 1:length( output_j ), function( .dummy.null = NULL ){ 
  matrix(nrow = nsim, ncol = length( var_labels ), dimnames = list(NULL, var_labels)) })
for( out.index in 1:length( output_j )){
  outmats[[ out.index ]][,]  <- t(vapply( output_j[[ out.index ]]@out, function( .list ){ unlist( .list[1:9] )}, numeric(9)))
}
cat(paste0("Success! \nElapsed time: \n")); print( proc.time() - ptm );
```

*******************************************************************************

# Processing Results: Model 1, Output 1
```{r evaluate-output-mod-1-out-1, eval = TRUE, cache = TRUE, echo = FALSE}
out.index = 1; # considering output 1 
cat("Here are the simulation conditions (settings) for model 1, output 1:") 
unlist(model( batch1 )[[1]]@params)
cat("Here is the method name for model 1, output 1:") 
method_names_output[ out.index ]
cat("Note that outcome_marginal_prevalence = 0.10 & trial_size = 32") 
outmatrix <- outmats[[ out.index ]]
cat("Here are first several rows") 
head( outmatrix )
```

For instance, rows 3 and 6 have large estimates and standard errors.
We plot estimates vs. SEs to identify any trends.

```{r fig-est-vs-se, eval = TRUE, cache = TRUE, echo = FALSE}
# plot ests vs. SEs without any classifiers
plot( outmatrix[,"est"], outmatrix[,"se"], xlab = "Estimate", ylab = "Standard Error (SE)",
                    main = paste0("Estimates vs. SEs, logistic regression \nModel 1, Output: ", 
                                  method_names_output[ out.index ]))
```

Using the previous method of excluding simulation conditions where the standard errors were greater than 1000, 
this classifier fails to identify simulations with estimates greater than 40 in both directions, but with small SEs.
What are these points, and what simulated outcomes (and predictors) do they correspond to?

```{r fig-est-vs-se-colored-scatter, eval = TRUE, cache = TRUE, echo = FALSE}
out.index = 1
# TODO( michael ): plot estimates vs. SEs using 'SE greater than 1k' classifier
plot( outmatrix[, "est"], outmatrix[, "se"], xlab = "Estimate", ylab = "Standard Error (SE)",
      col = ifelse( outmatrix[, "se"] > 1000, "red", "black"),
      main = paste0("Estimates vs. SEs, logistic regression \nModel 1, Output: ", method_names_output[ out.index ]))

classifier_1 <- outmatrix[, "se"] > 1000;
cat("Mean number of simulations removed under classifier 1 (SE > 1000):")
mean( classifier_1 )
classifier_2 <- outmatrix[, "se"] > 1000 | abs( outmatrix[, "est"] ) > 40;
cat("Mean number of simulations removed under classifier 2 (SE > 1000 | abs(est) > 40):")
mean( classifier_2 )
class2_newpoints <- which( classifier_2 & ! classifier_1 ) 
cols.classifier <- ifelse( outmatrix[, "se"] > 1000, "red",
                            ifelse(abs( outmatrix[, "est"] ) > 40, "blue", "black" ))

# Compare to new classifier
plot( outmatrix[, "est"], outmatrix[, "se"], xlab = "Estimate", ylab = "Standard Error (SE)",
      col = cols.classifier,
      main = paste0("Estimates vs. SEs, logistic regression \nModel 1, Output: ", method_names_output[ out.index ]))
legend("topright", legend = c("SE less than 1000, abs(est) greater than 40",
                              "SE greater than 1000",
                              "Valid estimates"),
       col = c("red", "blue", "black"), pch = 16)
```

What do these simulations correspond to?

```{r back-to-simulations-small-se-big-ests, eval = TRUE, cache = TRUE, echo = TRUE}
# Get the indices of the simulations
classifier_1 <- outmatrix[, "se"] > 1000;
class1_points <- which( classifier_1 )
## P
mean( classifier_1 )
classifier_2 <- outmatrix[, "se"] > 1000 | abs( outmatrix[, "est"] ) > 40;
mean( classifier_2 )
class2_newpoints <- which( classifier_2 & ! classifier_1 )



# Extract the corresponding simulations with the outcome measures
sims_batch1 <- output( batch1, methods = "CR" )[[1]]

for( index in class2_newpoints[1:10] ){
print( with( sims_batch1@out[[ index ]], table(Z, Y)) )
}

## Example draw with est > 40 and SE < 1k
with( sims_batch1@out[[ class2_newpoints[1] ]], table(Z, Y))
## Example draw with est < 40 and SE > 1k
with( sims_batch1@out[[ class1_points[1] ]], table(Z, Y))

# What accounts for the difference? 
# Maybe: look at the prognostic factors (X)

```

Not sure how to distinguish between the two cases (any input appreciated)
*******************************************************************************
      
```{r fig-est-vs-se-colored-hist, eval = FALSE, cache = TRUE, echo = FALSE}


h1 <- hist( outmatrix[ classifier_1 , "est"], plot=FALSE )
h2 <- hist( outmatrix[ !classifier_1 , "est"], plot=FALSE )
h2$counts = - h2$counts
hmax = max( h1$counts )
hmin = min( h2$counts )
X = c( h1$breaks, h2$breaks )
xmax = max( X )
xmin = min( X )
plot(h1, ylim=c(hmin, hmax), col="green", xlim=c(xmin, xmax),
     main = "Estimates classified by SE greater than 1000 (green)")
lines(h2, col="blue")
```



*******************************************************************************

